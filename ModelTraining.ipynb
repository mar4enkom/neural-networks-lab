{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1e14d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.ops as ops \n",
    "from torchvision.ops import roi_pool\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8535a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'filename': [],\n",
    "    'width': [],\n",
    "    'height': [],\n",
    "    'class': [],\n",
    "    'xmin': [],\n",
    "    'ymin': [],\n",
    "    'xmax': [],\n",
    "    'ymax': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2e10652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_image_dimensions(file_path):\n",
    "    if not os.path.isfile(file_path):\n",
    "        return None, None\n",
    "    with Image.open(file_path) as img:\n",
    "        width, height = img.size\n",
    "    return width, height\n",
    "\n",
    "def get_xml_image_dimensions(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    size = root.find('size')\n",
    "    if size is not None:\n",
    "        width = size.find('width').text\n",
    "        height = size.find('height').text\n",
    "        if width and height:\n",
    "            return int(width), int(height)\n",
    "    return 0, 0  \n",
    "\n",
    "\n",
    "def get_image_dimensions(xml_file, image_file_path):\n",
    "    width, height = get_xml_image_dimensions(xml_file)\n",
    "    \n",
    "    if width == 0 or height == 0:\n",
    "        width, height = get_file_image_dimensions(image_file_path)\n",
    "        \n",
    "    return width, height\n",
    "\n",
    "\n",
    "def parse_xml(xml_file, image_file_path):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    filename = root.find('filename').text\n",
    "    \n",
    "    width, height = get_image_dimensions(xml_file, image_file_path)\n",
    "\n",
    "\n",
    "    for obj in root.iter('object'):\n",
    "        obj_class = obj.find('name').text\n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text)\n",
    "        ymin = int(bbox.find('ymin').text)\n",
    "        xmax = int(bbox.find('xmax').text)\n",
    "        ymax = int(bbox.find('ymax').text)\n",
    "\n",
    "        data['filename'].append(filename)\n",
    "        data['width'].append(width)\n",
    "        data['height'].append(height)\n",
    "        data['class'].append(obj_class)\n",
    "        data['xmin'].append(xmin)\n",
    "        data['ymin'].append(ymin)\n",
    "        data['xmax'].append(xmax)\n",
    "        data['ymax'].append(ymax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acdeeb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitDataset(Dataset):\n",
    "    def __init__(self, data_dir, transforms=None, image_size=(416, 416)):\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_size = image_size \n",
    "        \n",
    "        self.images = [f for f in os.listdir(data_dir) if f.endswith('.jpg')]\n",
    "        \n",
    "        for image_file in self.images:\n",
    "            xml_file = image_file.replace('.jpg', '.xml')\n",
    "            xml_path = os.path.join(data_dir, xml_file)\n",
    "            image_path = os.path.join(data_dir, image_file)\n",
    "            if os.path.exists(xml_path):\n",
    "                parse_xml(xml_path, image_path)\n",
    "        \n",
    "        self.dataframe = pd.DataFrame(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def class_to_label(self, class_name):\n",
    "        class_mapping = {'apple': 1, 'banana': 2, 'orange': 3, 'mixed': 4}\n",
    "        return class_mapping.get(class_name, 0) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.images[idx]\n",
    "        image_path = os.path.join(self.data_dir, image_name)\n",
    "\n",
    "    # Завантажуємо зображення\n",
    "        image = cv2.imread(image_path)\n",
    "    \n",
    "    # Перетворюємо в RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "    \n",
    "        image_data = self.dataframe[self.dataframe['filename'] == image_name]\n",
    "        for _, row in image_data.iterrows():\n",
    "            xmin = row['xmin']\n",
    "            ymin = row['ymin']\n",
    "            xmax = row['xmax']\n",
    "            ymax = row['ymax']\n",
    "            label = self.class_to_label(row['class'])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(label)\n",
    "    \n",
    "        orig_height, orig_width = image.shape[:2]\n",
    "    \n",
    "    # Приведення всіх зображень до одного розміру\n",
    "        image = cv2.resize(image, self.image_size)\n",
    "    \n",
    "    # Пропорційне масштабування bounding boxes\n",
    "        scale_x = self.image_size[0] / orig_width\n",
    "        scale_y = self.image_size[1] / orig_height\n",
    "        boxes = [[xmin * scale_x, ymin * scale_y, xmax * scale_x, ymax * scale_y] for xmin, ymin, xmax, ymax in boxes]\n",
    "    \n",
    "        boxes = [[xmin / self.image_size[0], ymin / self.image_size[1], xmax / self.image_size[0], ymax / self.image_size[1]] for xmin, ymin, xmax, ymax in boxes]\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
    "            image = transformed['image']\n",
    "            boxes = torch.as_tensor(transformed['bboxes'], dtype=torch.float32)\n",
    "\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "    \n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "    \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62420d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 512]) tensor([[0.2032, 0.0000, 0.9268, 0.7233],\n",
      "        [0.5275, 0.4868, 1.0000, 0.9743]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.1287, 0.3500, 0.8188, 0.8067]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.2149, 0.1188, 0.5026, 0.4992],\n",
      "        [0.4612, 0.1452, 0.7340, 0.5370],\n",
      "        [0.5122, 0.4523, 0.8327, 0.8599]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.2931, 0.0000, 0.9514, 0.3156]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0158, 0.1696, 0.8481, 0.9178],\n",
      "        [0.3989, 0.1948, 0.8090, 0.8462]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.3070, 0.1860, 0.7578, 0.6324],\n",
      "        [0.2309, 0.5353, 0.6614, 0.9070],\n",
      "        [0.7102, 0.6140, 0.9878, 0.9460],\n",
      "        [0.5756, 0.2344, 0.9358, 0.6268],\n",
      "        [0.0022, 0.0280, 0.4305, 0.5052]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 512]) tensor([[0.0833, 0.1400, 0.9167, 0.9900]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0830, 0.0672, 0.9750, 0.8857]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0249, 0.1768, 0.6795, 0.9972]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0257, 0.1812, 0.5213, 0.8900],\n",
      "        [0.5008, 0.1767, 0.8586, 0.6693],\n",
      "        [0.3263, 0.4699, 0.7141, 0.9636]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.4571, 0.2979, 0.8496, 0.8917],\n",
      "        [0.0695, 0.2604, 0.5108, 0.9038]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.6325, 0.4916, 0.8375, 0.7730],\n",
      "        [0.5662, 0.0826, 0.8050, 0.4503],\n",
      "        [0.4212, 0.0732, 0.5788, 0.7992],\n",
      "        [0.1963, 0.2946, 0.5263, 0.9099],\n",
      "        [0.3200, 0.0713, 0.5125, 0.7786]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0217, 0.1745, 0.6200, 0.9445],\n",
      "        [0.4817, 0.1388, 0.9836, 0.8177]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.3857, 0.0086, 0.9386, 0.5600],\n",
      "        [0.0529, 0.2143, 0.2657, 0.6771],\n",
      "        [0.1771, 0.3486, 0.9514, 0.8886]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.4120, 0.4160, 0.8960, 0.9880],\n",
      "        [0.3880, 0.0040, 0.8120, 0.4600],\n",
      "        [0.0040, 0.2480, 0.5240, 0.7480],\n",
      "        [0.0040, 0.0080, 0.4440, 0.2800],\n",
      "        [0.1200, 0.8040, 0.6080, 1.0000]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0545, 0.1983, 0.8005, 1.0000]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.4208, 0.2703, 0.8615, 0.9297],\n",
      "        [0.2448, 0.1797, 0.5823, 0.6891]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.2219, 0.0867, 0.7344, 0.7939],\n",
      "        [0.2937, 0.2670, 0.9156, 0.8689],\n",
      "        [0.3156, 0.0726, 0.8031, 0.5293]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.1042, 0.2111, 0.6433, 0.7889]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.3354, 0.2000, 0.7992, 0.6396]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0956, 0.0491, 0.7778, 0.8248]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.1804, 0.0011, 0.9912, 0.6933]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0691, 0.2189, 0.4711, 0.8411],\n",
      "        [0.4841, 0.2048, 0.9096, 0.8855]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.3574, 0.3748, 0.7677, 0.7971],\n",
      "        [0.2451, 0.2433, 0.6502, 0.6756]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0811, 0.3715, 0.6214, 0.9192],\n",
      "        [0.3680, 0.2492, 0.8340, 0.8104]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0483, 0.0961, 0.8662, 0.9807]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.1518, 0.4061, 0.4569, 0.7032],\n",
      "        [0.4122, 0.4510, 0.6998, 0.7186],\n",
      "        [0.0713, 0.3868, 0.3365, 0.6333],\n",
      "        [0.7305, 0.4039, 0.9666, 0.6586]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.1223, 0.1749, 0.7792, 0.8341]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.5683, 0.4015, 0.9140, 0.8661],\n",
      "        [0.1169, 0.0024, 0.7272, 0.5200],\n",
      "        [0.0886, 0.1413, 0.6858, 0.8503]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.2355, 0.0903, 0.6826, 0.8149]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.1200, 0.2680, 0.8280, 0.9360]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.3536, 0.1801, 0.8650, 0.6774]])\n",
      "torch.Size([3, 512, 512]) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0090, 0.2392, 0.6103, 1.0000],\n",
      "        [0.3910, 0.0911, 0.9962, 1.0000]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.1015, 0.1926, 0.5413, 0.8410],\n",
      "        [0.4743, 0.1141, 0.9318, 0.7663]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.1800, 0.2080, 0.8520, 0.8880]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.3075, 0.2198, 0.9410, 0.9462]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0636, 0.0818, 0.9727, 0.9364]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.4891, 0.1785, 0.7469, 0.4061],\n",
      "        [0.1984, 0.1727, 0.5000, 0.4329],\n",
      "        [0.1141, 0.4271, 0.9656, 0.7223]])\n",
      "torch.Size([3, 512, 512]) tensor([[0., 0., 1., 1.]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.1760, 0.1072, 0.8258, 0.9156],\n",
      "        [0.0675, 0.1666, 0.6538, 1.0000],\n",
      "        [0.2377, 0.0656, 0.8706, 0.6944]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 512]) tensor([[0.4104, 0.3447, 1.0000, 1.0000]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0077, 0.3471, 0.9369, 0.8168]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.2896, 0.1253, 0.9035, 0.9896]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0015, 0.2375, 0.7755, 0.9389]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.2517, 0.1491, 0.9067, 0.9814]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0785, 0.6733, 0.9607, 0.9030]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.1463, 0.2129, 0.4642, 0.7479],\n",
      "        [0.5884, 0.3595, 0.8821, 0.7905],\n",
      "        [0.2653, 0.4634, 0.5568, 0.8756],\n",
      "        [0.7358, 0.3152, 0.9884, 0.7717]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0460, 0.1100, 0.9160, 0.9940]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.1299, 0.2346, 0.4851, 0.7106],\n",
      "        [0.5179, 0.2175, 0.9000, 0.6815]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0000, 0.0000, 0.6519, 1.0000]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0270, 0.0335, 0.9717, 0.9411]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.1425, 0.1823, 0.8798, 1.0000]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.3696, 0.1629, 0.8752, 1.0000],\n",
      "        [0.1312, 0.1400, 0.7264, 0.6400],\n",
      "        [0.1616, 0.1286, 0.7472, 0.8686],\n",
      "        [0.1472, 0.1457, 0.6960, 0.7286]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0453, 0.1044, 0.6006, 0.9203]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.1097, 0.4600, 0.3016, 0.7886],\n",
      "        [0.3355, 0.5171, 0.5468, 0.8743],\n",
      "        [0.4258, 0.1829, 0.5968, 0.4543],\n",
      "        [0.5339, 0.0029, 0.7516, 0.3000],\n",
      "        [0.6774, 0.1314, 0.8726, 0.4629],\n",
      "        [0.5226, 0.2971, 0.7065, 0.6029]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.2594, 0.2350, 0.6450, 0.7517],\n",
      "        [0.6187, 0.3083, 0.9862, 0.8117]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0022, 0.4630, 0.9283, 0.9283]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0153, 0.0197, 0.5624, 0.7495]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.1080, 0.2683, 0.8639, 0.9273]])\n",
      "torch.Size([3, 512, 512]) tensor([[0.0903, 0.0375, 0.9625, 0.4000],\n",
      "        [0.1667, 0.2208, 0.9819, 0.8667],\n",
      "        [0.3875, 0.0292, 1.0000, 0.6417]])\n"
     ]
    }
   ],
   "source": [
    "transform = A.Compose([\n",
    "    \n",
    "    # Дзеркально відображає зобреження, щоб в подальшому модель звикала до симетрії(обʼєкт може бути як ліворуч так і праворуч)\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    # Перевертає зображення щоб якщо обʼєкти були перевернутими, або нахиленимим, модель всеодно їх впізнавала\n",
    "    A.Rotate(limit=15, p=0.5),\n",
    "    # Допомагає моделі розпізнавати обʼєкти не залежно від умов освітлення\n",
    "    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2, p=0.5),\n",
    "    # моделі в PyTorch очікують вхідні дані у вигляді тензорів\n",
    "    ToTensorV2(p=1.0),\n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['labels']))\n",
    "\n",
    "dataset = FruitDataset(data_dir='./datasets/train_zip/train', transforms=transform, image_size=(512, 512))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "for images, targets in dataloader:\n",
    "    print(images[0].shape, targets[0]['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95fea977",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_loader\u001b[49m:\n\u001b[1;32m      2\u001b[0m     images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(images)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "for images, targets in train_loader:\n",
    "    images = torch.stack(images).to(device)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    \n",
    "    bbox_preds, class_preds = model(images)\n",
    "\n",
    "    print(f\"Розмірність передбачених bounding boxes: {bbox_preds.shape}\")\n",
    "    print(f\"Розмірність реальних bounding boxes: {[t['boxes'].shape for t in targets]}\")\n",
    "\n",
    "    bboxes = torch.cat([t['boxes'] for t in targets], dim=0)\n",
    "    labels = torch.cat([t['labels'] for t in targets], dim=0)\n",
    "\n",
    "    print(f\"Зведена розмірність реальних bounding boxes: {bboxes.shape}\")\n",
    "    print(f\"Зведена розмірність передбачених bounding boxes: {bbox_preds.shape}\")\n",
    "    \n",
    "    if bbox_preds.shape == bboxes.shape:\n",
    "        bbox_loss = bbox_loss_fn(bbox_preds, bboxes)\n",
    "        class_loss = class_loss_fn(class_preds, labels)\n",
    "        loss = bbox_loss + class_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        print(\"Розмірності не збігаються.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26fe08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class AlexNetObjectDetectorWithAnchors(nn.Module):\n",
    "    def __init__(self, num_classes, num_anchors=9):\n",
    "        super(AlexNetObjectDetectorWithAnchors, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = models.alexnet(pretrained=True).features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "\n",
    "        self.bbox_head = nn.Linear(4096, num_anchors * 4)\n",
    "        \n",
    "        self.class_head = nn.Linear(4096, num_anchors * num_classes)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        bbox_coords = self.bbox_head(x)\n",
    "        bbox_coords = bbox_coords.view(-1, self.num_anchors, 4)\n",
    "\n",
    "        class_scores = self.class_head(x)\n",
    "        class_scores = class_scores.view(-1, self.num_anchors, self.num_classes)\n",
    "        return bbox_coords, class_scores\n",
    "\n",
    "num_classes = 4 \n",
    "model = AlexNetObjectDetectorWithAnchors(num_classes=num_classes, num_anchors=9)\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 512, 512) \n",
    "bbox_coords, class_scores = model(dummy_input)\n",
    "\n",
    "print(\"Bounding Box Coordinates:\", bbox_coords)\n",
    "print(\"Class Scores:\", class_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4fa4fcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AlexNetObjectDetectorWithAnchors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m \n\u001b[1;32m      9\u001b[0m num_anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m \n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAlexNetObjectDetectorWithAnchors\u001b[49m(num_classes\u001b[38;5;241m=\u001b[39mnum_classes, num_anchors\u001b[38;5;241m=\u001b[39mnum_anchors)\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     13\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AlexNetObjectDetectorWithAnchors' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_classes = 4 \n",
    "num_anchors = 9 \n",
    "model = AlexNetObjectDetectorWithAnchors(num_classes=num_classes, num_anchors=num_anchors)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "bbox_loss_fn = nn.MSELoss() \n",
    "class_loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_bbox_loss = 0.0\n",
    "    total_class_loss = 0.0\n",
    "    \n",
    "    for images, targets in train_loader:\n",
    "        images = torch.stack(images).to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        bbox_preds, class_preds = model(images)\n",
    "        \n",
    "        batch_bbox_loss = 0.0\n",
    "        batch_class_loss = 0.0\n",
    "\n",
    "        for i, target in enumerate(targets):\n",
    "            num_boxes = target['boxes'].shape[0]\n",
    "            if num_boxes > num_anchors:\n",
    "                bboxes = target['boxes'][:num_anchors]\n",
    "                labels = target['labels'][:num_anchors]\n",
    "            else:\n",
    "                padding_boxes = torch.zeros((num_anchors - num_boxes, 4), device=device)\n",
    "                padding_labels = torch.zeros(num_anchors - num_boxes, dtype=torch.long, device=device)\n",
    "                bboxes = torch.cat([target['boxes'], padding_boxes], dim=0)\n",
    "                labels = torch.cat([target['labels'], padding_labels], dim=0)\n",
    "                \n",
    "            batch_bbox_loss += bbox_loss_fn(bbox_preds[i], bboxes)\n",
    "            batch_class_loss += class_loss_fn(class_preds[i], labels)\n",
    "        \n",
    "        batch_bbox_loss /= len(targets)\n",
    "        batch_class_loss /= len(targets)\n",
    "\n",
    "        loss = batch_bbox_loss + batch_class_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_bbox_loss += batch_bbox_loss.item()\n",
    "        total_class_loss += batch_class_loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], BBox Loss: {total_bbox_loss/len(train_loader):.4f}, Class Loss: {total_class_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Тренування завершено.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2e7b45a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тестова BBox Loss: 0.0672\n",
      "Точність класифікації: 61.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval() \n",
    "    total_bbox_loss = 0.0\n",
    "    total_class_accuracy = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    bbox_loss_fn = nn.MSELoss() \n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for images, targets in test_loader:\n",
    "            images = torch.stack(images).to(device)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            bbox_preds, class_preds = model(images)\n",
    "\n",
    "            batch_bbox_loss = 0.0\n",
    "            batch_class_accuracy = 0.0\n",
    "\n",
    "            for i, target in enumerate(targets):\n",
    "                num_boxes = target['boxes'].shape[0]\n",
    "                if num_boxes > num_anchors:\n",
    "                    bboxes = target['boxes'][:num_anchors]\n",
    "                    labels = target['labels'][:num_anchors]\n",
    "                else:\n",
    "                    padding_boxes = torch.zeros((num_anchors - num_boxes, 4), device=device)\n",
    "                    padding_labels = torch.zeros(num_anchors - num_boxes, dtype=torch.long, device=device)\n",
    "                    bboxes = torch.cat([target['boxes'], padding_boxes], dim=0)\n",
    "                    labels = torch.cat([target['labels'], padding_labels], dim=0)\n",
    "                \n",
    "                batch_bbox_loss += bbox_loss_fn(bbox_preds[i], bboxes).item()\n",
    "\n",
    "                _, predicted_classes = torch.max(class_preds[i], 1)\n",
    "                all_true_labels.extend(labels.cpu().numpy())\n",
    "                all_pred_labels.extend(predicted_classes.cpu().numpy())\n",
    "\n",
    "            total_bbox_loss += batch_bbox_loss / len(targets)\n",
    "            total_samples += 1\n",
    "\n",
    "    avg_bbox_loss = total_bbox_loss / total_samples\n",
    "\n",
    "    class_accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "\n",
    "    print(f\"Тестова BBox Loss: {avg_bbox_loss:.4f}\")\n",
    "    print(f\"Точність класифікації: {class_accuracy * 100:.2f}%\")\n",
    "\n",
    "test_dataset = FruitDataset(data_dir='/Users/matvejzasadko/Downloads/All/Study/NNetworks/Lb1/archive/test_zip/test', transforms=transform, image_size=(512, 512))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "evaluate_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a3708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb875b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matvejzasadko/.pyenv/versions/3.8.13/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/matvejzasadko/.pyenv/versions/3.8.13/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding Box Coordinates: tensor([[[-1.7623e-02, -1.4542e-03, -7.1488e-03, -6.4496e-03],\n",
      "         [-1.8502e-02, -3.9880e-03, -2.2023e-02,  1.1719e-02],\n",
      "         [ 1.3590e-02,  8.3991e-03, -5.2298e-04, -1.2157e-03],\n",
      "         [ 4.5781e-03,  1.6443e-02, -1.7070e-03, -1.1228e-02],\n",
      "         [ 1.3366e-02,  1.3399e-04, -1.4536e-03, -6.0596e-03],\n",
      "         [-2.0067e-02, -8.9800e-05, -8.4977e-03,  6.0467e-03],\n",
      "         [-9.0072e-03, -1.5734e-02, -1.2572e-02,  1.3781e-02],\n",
      "         [ 1.2443e-02,  7.8619e-04, -9.0144e-04, -2.1730e-03],\n",
      "         [-1.3206e-02,  1.2758e-02, -1.1566e-02, -8.8705e-03]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "Class Scores: tensor([[[ 0.0016,  0.0119, -0.0045,  0.0083],\n",
      "         [ 0.0141, -0.0004, -0.0059, -0.0159],\n",
      "         [-0.0107,  0.0076, -0.0028,  0.0066],\n",
      "         [-0.0173,  0.0098,  0.0153,  0.0020],\n",
      "         [ 0.0084, -0.0009,  0.0077, -0.0178],\n",
      "         [ 0.0061,  0.0053,  0.0114, -0.0139],\n",
      "         [ 0.0076,  0.0196, -0.0102, -0.0050],\n",
      "         [ 0.0133, -0.0023, -0.0052,  0.0051],\n",
      "         [-0.0025,  0.0039,  0.0144, -0.0191]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.ops import roi_pool\n",
    "\n",
    "class RCNN(nn.Module):\n",
    "    def __init__(self, num_classes, num_rois=9):\n",
    "        super(RCNN, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = models.alexnet(pretrained=True).features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        \n",
    "        self.bbox_head = nn.Linear(4096, num_rois * 4)\n",
    "        \n",
    "        self.class_head = nn.Linear(4096, num_rois * num_classes)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_rois = num_rois\n",
    "\n",
    "    def forward(self, x, rois):\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        pooled_features = roi_pool(features, rois, output_size=(6, 6))\n",
    "        \n",
    "        x = self.avgpool(pooled_features)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        bbox_coords = self.bbox_head(x)\n",
    "        bbox_coords = bbox_coords.view(-1, self.num_rois, 4)\n",
    "\n",
    "        class_scores = self.class_head(x)\n",
    "        class_scores = class_scores.view(-1, self.num_rois, self.num_classes)\n",
    "        \n",
    "        return bbox_coords, class_scores\n",
    "\n",
    "num_classes = 4 \n",
    "num_rois = 9\n",
    "model = RCNN(num_classes=num_classes, num_rois=num_rois)\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 512, 512) \n",
    "dummy_rois = torch.tensor([[0, 50, 50, 400, 400]], dtype=torch.float)  # Example ROI [batch_index, x1, y1, x2, y2]\n",
    "\n",
    "bbox_coords, class_scores = model(dummy_input, dummy_rois)\n",
    "\n",
    "print(\"Bounding Box Coordinates:\", bbox_coords)\n",
    "print(\"Class Scores:\", class_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1d760a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], BBox Loss: 0.0464, Class Loss: 0.6007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], BBox Loss: 0.0339, Class Loss: 0.4911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], BBox Loss: 0.0352, Class Loss: 0.4882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], BBox Loss: 0.0333, Class Loss: 0.4894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], BBox Loss: 0.0318, Class Loss: 0.4876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], BBox Loss: 0.0315, Class Loss: 0.4844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], BBox Loss: 0.0326, Class Loss: 0.4906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], BBox Loss: 0.0307, Class Loss: 0.4809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], BBox Loss: 0.0312, Class Loss: 0.4903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], BBox Loss: 0.0331, Class Loss: 0.4782\n",
      "Тренування завершено.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_classes = 4 \n",
    "num_rois = 9\n",
    "model = RCNN(num_classes=num_classes, num_rois=num_rois)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "bbox_loss_fn = nn.MSELoss() \n",
    "class_loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_bbox_loss = 0.0\n",
    "    total_class_loss = 0.0\n",
    "    \n",
    "    for images, targets in dataloader: \n",
    "        images = torch.stack(images).to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        rois = []\n",
    "        for idx in range(len(images)):\n",
    "            roi = torch.tensor([[idx, 50, 50, 400, 400],\n",
    "                                [idx, 100, 100, 300, 300]], dtype=torch.float, device=device)\n",
    "            rois.append(roi)\n",
    "        \n",
    "        rois = torch.cat(rois, dim=0) \n",
    "        \n",
    "        bbox_preds, class_preds = model(images, rois)\n",
    "        \n",
    "        batch_bbox_loss = 0.0\n",
    "        batch_class_loss = 0.0\n",
    "\n",
    "        for i, target in enumerate(targets):\n",
    "            num_boxes = target['boxes'].shape[0]\n",
    "            if num_boxes > num_rois:\n",
    "                bboxes = target['boxes'][:num_rois]\n",
    "                labels = target['labels'][:num_rois]\n",
    "            else:\n",
    "                padding_boxes = torch.zeros((num_rois - num_boxes, 4), device=device)\n",
    "                padding_labels = torch.zeros(num_rois - num_boxes, dtype=torch.long, device=device)\n",
    "                bboxes = torch.cat([target['boxes'], padding_boxes], dim=0)\n",
    "                labels = torch.cat([target['labels'], padding_labels], dim=0)\n",
    "                \n",
    "            batch_bbox_loss += bbox_loss_fn(bbox_preds[i], bboxes)\n",
    "            batch_class_loss += class_loss_fn(class_preds[i], labels)\n",
    "        \n",
    "        batch_bbox_loss /= len(targets)\n",
    "        batch_class_loss /= len(targets)\n",
    "\n",
    "        loss = batch_bbox_loss + batch_class_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_bbox_loss += batch_bbox_loss.item()\n",
    "        total_class_loss += batch_class_loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], BBox Loss: {total_bbox_loss/len(dataloader):.4f}, Class Loss: {total_class_loss/len(dataloader):.4f}\")\n",
    "\n",
    "print(\"Тренування завершено.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23a4fc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тестова BBox Loss: 0.0812\n",
      "Точність класифікації: 61.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_dataset = FruitDataset(data_dir='/Users/matvejzasadko/Downloads/All/Study/NNetworks/Lb1/archive/test_zip/test', \n",
    "                            transforms=transform, \n",
    "                            image_size=(512, 512))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "def evaluate_model(model, test_loader, device, num_rois=9):\n",
    "    model.eval()  \n",
    "    total_bbox_loss = 0.0\n",
    "    all_true_labels = []\n",
    "    all_pred_labels = []\n",
    "\n",
    "    bbox_loss_fn = nn.MSELoss() \n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for images, targets in test_loader:\n",
    "            images = torch.stack(images).to(device)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            rois = []\n",
    "            for idx in range(len(images)):\n",
    "                roi = torch.tensor([[idx, 50, 50, 400, 400],\n",
    "                                    [idx, 100, 100, 300, 300]], dtype=torch.float, device=device)\n",
    "                rois.append(roi)\n",
    "            \n",
    "            rois = torch.cat(rois, dim=0)  \n",
    "\n",
    "            bbox_preds, class_preds = model(images, rois)\n",
    "\n",
    "            batch_bbox_loss = 0.0\n",
    "\n",
    "            for i, target in enumerate(targets):\n",
    "                num_boxes = target['boxes'].shape[0]\n",
    "                if num_boxes > num_rois:\n",
    "                    bboxes = target['boxes'][:num_rois]\n",
    "                    labels = target['labels'][:num_rois]\n",
    "                else:\n",
    "                    padding_boxes = torch.zeros((num_rois - num_boxes, 4), device=device)\n",
    "                    padding_labels = torch.zeros(num_rois - num_boxes, dtype=torch.long, device=device)\n",
    "                    bboxes = torch.cat([target['boxes'], padding_boxes], dim=0)\n",
    "                    labels = torch.cat([target['labels'], padding_labels], dim=0)\n",
    "\n",
    "                batch_bbox_loss += bbox_loss_fn(bbox_preds[i], bboxes).item()\n",
    "\n",
    "                _, predicted_classes = torch.max(class_preds[i], 1)\n",
    "                all_true_labels.extend(labels.cpu().numpy())\n",
    "                all_pred_labels.extend(predicted_classes.cpu().numpy())\n",
    "\n",
    "            total_bbox_loss += batch_bbox_loss / len(targets)\n",
    "\n",
    "    avg_bbox_loss = total_bbox_loss / len(test_loader)\n",
    "\n",
    "    class_accuracy = accuracy_score(all_true_labels, all_pred_labels)\n",
    "\n",
    "    print(f\"Тестова BBox Loss: {avg_bbox_loss:.4f}\")\n",
    "    print(f\"Точність класифікації: {class_accuracy * 100:.2f}%\")\n",
    "\n",
    "evaluate_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6901c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca521611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206324c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee14895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd0e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6694652d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879e529d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cac992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a04f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1379354c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948d0de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac64a64a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87596932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207db45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
