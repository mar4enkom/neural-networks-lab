{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45632e9e-370c-4eba-bca0-60acbfa5bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67388b7b-55b6-4d8a-9baf-9575fac169b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'filename': [],\n",
    "    'width': [],\n",
    "    'height': [],\n",
    "    'class': [],\n",
    "    'xmin': [],\n",
    "    'ymin': [],\n",
    "    'xmax': [],\n",
    "    'ymax': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edea2209-a6fd-4f18-af97-c76133c77785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_image_dimensions(file_path):\n",
    "    if not os.path.isfile(file_path):\n",
    "        return None, None\n",
    "    with Image.open(file_path) as img:\n",
    "        width, height = img.size\n",
    "    return width, height\n",
    "\n",
    "def get_xml_image_dimensions(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    size = root.find('size')\n",
    "    if size is not None:\n",
    "        width = size.find('width').text\n",
    "        height = size.find('height').text\n",
    "        if width and height:\n",
    "            return int(width), int(height)\n",
    "    return 0, 0  \n",
    "\n",
    "\n",
    "def get_image_dimensions(xml_file, image_file_path):\n",
    "    width, height = get_xml_image_dimensions(xml_file)\n",
    "    \n",
    "    if width == 0 or height == 0:\n",
    "        width, height = get_file_image_dimensions(image_file_path)\n",
    "        \n",
    "    return width, height\n",
    "\n",
    "\n",
    "def parse_xml(xml_file, image_file_path):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    filename = root.find('filename').text\n",
    "    \n",
    "    width, height = get_image_dimensions(xml_file, image_file_path)\n",
    "\n",
    "\n",
    "    for obj in root.iter('object'):\n",
    "        obj_class = obj.find('name').text\n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text)\n",
    "        ymin = int(bbox.find('ymin').text)\n",
    "        xmax = int(bbox.find('xmax').text)\n",
    "        ymax = int(bbox.find('ymax').text)\n",
    "\n",
    "        data['filename'].append(filename)\n",
    "        data['width'].append(width)\n",
    "        data['height'].append(height)\n",
    "        data['class'].append(obj_class)\n",
    "        data['xmin'].append(xmin)\n",
    "        data['ymin'].append(ymin)\n",
    "        data['xmax'].append(xmax)\n",
    "        data['ymax'].append(ymax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a5fa870-97e7-417f-9854-d7f81503f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitDataset(Dataset):\n",
    "    def __init__(self, data_dir, transforms=None, image_size=(224, 224)):\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_size = image_size \n",
    "        \n",
    "        self.images = [f for f in os.listdir(data_dir) if f.endswith('.jpg')]\n",
    "        \n",
    "        for image_file in self.images:\n",
    "            xml_file = image_file.replace('.jpg', '.xml')\n",
    "            xml_path = os.path.join(data_dir, xml_file)\n",
    "            image_path = os.path.join(data_dir, image_file)\n",
    "            if os.path.exists(xml_path):\n",
    "                parse_xml(xml_path, image_path)\n",
    "        \n",
    "        self.dataframe = pd.DataFrame(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def class_to_label(self, class_name):\n",
    "        class_mapping = {'apple': 0, 'banana': 1, 'orange': 2}  \n",
    "        return class_mapping.get(class_name, 0) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.images[idx]\n",
    "        image_path = os.path.join(self.data_dir, image_name)\n",
    "    \n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        # Convert BGR (OpenCV format) to RGB (PIL format)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        # Convert numpy array (OpenCV) to PIL Image\n",
    "        image = Image.fromarray(image)\n",
    "    \n",
    "        image_data = self.dataframe[self.dataframe['filename'] == image_name]\n",
    "        class_name = image_data.iloc[0]['class']\n",
    "        label = self.class_to_label(class_name)\n",
    "        \n",
    "        # Resize the image\n",
    "        image = image.resize(self.image_size)\n",
    "    \n",
    "        # Apply the transformations\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b8536cd-955b-470e-b6bf-eb5846bf7cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a85b8eb7-bd35-4272-905f-a8ad8d08b9e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parse_xml() missing 1 required positional argument: 'image_file_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load the dataset and DataLoader\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mFruitDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Initialize and modify AlexNet model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 32\u001b[0m, in \u001b[0;36mFruitDataset.__init__\u001b[0;34m(self, data_dir, transforms, image_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, image_file)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(xml_path):\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;66;03m# Assuming parse_xml returns label information as a dictionary\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m         label_data \u001b[38;5;241m=\u001b[39m \u001b[43mparse_xml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml_path\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure this function is implemented\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m: image_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: label_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "\u001b[0;31mTypeError\u001b[0m: parse_xml() missing 1 required positional argument: 'image_file_path'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "\n",
    "# Define class mapping and parameters\n",
    "class_mapping = {0: 'apple', 1: 'banana', 2: 'orange'}\n",
    "num_classes = len(class_mapping)\n",
    "data_dir = './datasets/train_zip/train'\n",
    "num_epochs = 4\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Load the dataset and DataLoader\n",
    "train_dataset = FruitDataset(data_dir, transforms=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize and modify AlexNet model\n",
    "model = models.alexnet(pretrained=True)\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize lists to store results\n",
    "all_predictions, all_labels = [], []\n",
    "incorrect_images, incorrect_labels, incorrect_preds = [], [], []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Optimization\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Store predictions and labels\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Save incorrect predictions\n",
    "        incorrect = predicted != labels\n",
    "        incorrect_indices = incorrect.nonzero(as_tuple=True)[0]\n",
    "        incorrect_images.extend(images[incorrect_indices].cpu())\n",
    "        incorrect_labels.extend(labels[incorrect_indices].cpu().tolist())\n",
    "        incorrect_preds.extend(predicted[incorrect_indices].cpu().tolist())\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "# Compute and plot confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions, labels=list(class_mapping.keys()))\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=list(class_mapping.values()), yticklabels=list(class_mapping.values()))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Display incorrect images\n",
    "if incorrect_images:\n",
    "    print(\"Displaying images with incorrect predictions:\")\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, img in enumerate(incorrect_images[:9]):  # Display up to 9 incorrect images\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        img = img.permute(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "        plt.imshow(img.numpy())\n",
    "        true_label = class_mapping[incorrect_labels[i]]\n",
    "        pred_label = class_mapping[incorrect_preds[i]]\n",
    "        plt.title(f\"True: {true_label}, Pred: {pred_label}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Training finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db38a228-a97a-433d-b2d5-4ea89ae8344f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
