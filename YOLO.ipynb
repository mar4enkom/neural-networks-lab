{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2684982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matvejzasadko/.pyenv/versions/3.8.13/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/matvejzasadko/.pyenv/versions/3.8.13/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.16). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.ops as ops \n",
    "from torchvision.ops import roi_pool\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1da54de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'filename': [],\n",
    "    'width': [],\n",
    "    'height': [],\n",
    "    'class': [],\n",
    "    'xmin': [],\n",
    "    'ymin': [],\n",
    "    'xmax': [],\n",
    "    'ymax': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc5a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_image_dimensions(file_path):\n",
    "    if not os.path.isfile(file_path):\n",
    "        return None, None\n",
    "    with Image.open(file_path) as img:\n",
    "        width, height = img.size\n",
    "    return width, height\n",
    "\n",
    "def get_xml_image_dimensions(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    size = root.find('size')\n",
    "    if size is not None:\n",
    "        width = size.find('width').text\n",
    "        height = size.find('height').text\n",
    "        if width and height:\n",
    "            return int(width), int(height)\n",
    "    return 0, 0  \n",
    "\n",
    "\n",
    "def get_image_dimensions(xml_file, image_file_path):\n",
    "    width, height = get_xml_image_dimensions(xml_file)\n",
    "    \n",
    "    if width == 0 or height == 0:\n",
    "        width, height = get_file_image_dimensions(image_file_path)\n",
    "        \n",
    "    return width, height\n",
    "\n",
    "\n",
    "def parse_xml(xml_file, image_file_path):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    filename = root.find('filename').text\n",
    "    \n",
    "    width, height = get_image_dimensions(xml_file, image_file_path)\n",
    "\n",
    "\n",
    "    for obj in root.iter('object'):\n",
    "        obj_class = obj.find('name').text\n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text)\n",
    "        ymin = int(bbox.find('ymin').text)\n",
    "        xmax = int(bbox.find('xmax').text)\n",
    "        ymax = int(bbox.find('ymax').text)\n",
    "\n",
    "        data['filename'].append(filename)\n",
    "        data['width'].append(width)\n",
    "        data['height'].append(height)\n",
    "        data['class'].append(obj_class)\n",
    "        data['xmin'].append(xmin)\n",
    "        data['ymin'].append(ymin)\n",
    "        data['xmax'].append(xmax)\n",
    "        data['ymax'].append(ymax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71867067",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitDataset(Dataset):\n",
    "    def __init__(self, data_dir, transforms=None, image_size=(224, 224)):\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_size = image_size \n",
    "        \n",
    "        self.images = [f for f in os.listdir(data_dir) if f.endswith('.jpg')]\n",
    "        \n",
    "        for image_file in self.images:\n",
    "            xml_file = image_file.replace('.jpg', '.xml')\n",
    "            xml_path = os.path.join(data_dir, xml_file)\n",
    "            image_path = os.path.join(data_dir, image_file)\n",
    "            if os.path.exists(xml_path):\n",
    "                parse_xml(xml_path, image_path)\n",
    "        \n",
    "        self.dataframe = pd.DataFrame(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def class_to_label(self, class_name):\n",
    "        class_mapping = {'apple': 0, 'banana': 1, 'orange': 2}\n",
    "        return class_mapping.get(class_name, 0) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.images[idx]\n",
    "        image_path = os.path.join(self.data_dir, image_name)\n",
    "\n",
    "    # Завантажуємо зображення\n",
    "        image = cv2.imread(image_path)\n",
    "    \n",
    "    # Перетворюємо в RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "    \n",
    "        image_data = self.dataframe[self.dataframe['filename'] == image_name]\n",
    "        for _, row in image_data.iterrows():\n",
    "            xmin = row['xmin']\n",
    "            ymin = row['ymin']\n",
    "            xmax = row['xmax']\n",
    "            ymax = row['ymax']\n",
    "            label = self.class_to_label(row['class'])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(label)\n",
    "    \n",
    "        orig_height, orig_width = image.shape[:2]\n",
    "    \n",
    "    # Приведення всіх зображень до одного розміру\n",
    "        image = cv2.resize(image, self.image_size)\n",
    "    \n",
    "    # Пропорційне масштабування bounding boxes\n",
    "        scale_x = self.image_size[0] / orig_width\n",
    "        scale_y = self.image_size[1] / orig_height\n",
    "        boxes = [[xmin * scale_x, ymin * scale_y, xmax * scale_x, ymax * scale_y] for xmin, ymin, xmax, ymax in boxes]\n",
    "    \n",
    "        boxes = [[xmin / self.image_size[0], ymin / self.image_size[1], xmax / self.image_size[0], ymax / self.image_size[1]] for xmin, ymin, xmax, ymax in boxes]\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=image, bboxes=boxes, labels=labels)\n",
    "            image = transformed['image']\n",
    "            boxes = torch.as_tensor(transformed['bboxes'], dtype=torch.float32)\n",
    "\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "    \n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "    \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97859781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) tensor([[0.0752, 0.2995, 0.8213, 0.7448],\n",
      "        [0.3545, 0.2747, 0.9893, 0.9818],\n",
      "        [0.0361, 0.2435, 0.7744, 0.6315]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.0623, 0.0806, 0.9466, 0.9403]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.5188, 0.3262, 0.9000, 0.7088],\n",
      "        [0.1312, 0.5325, 0.4913, 0.8637],\n",
      "        [0.2425, 0.3625, 0.5962, 0.6812]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.2594, 0.2350, 0.6450, 0.7517],\n",
      "        [0.6187, 0.3083, 0.9862, 0.8117]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.5446, 0.0688, 0.9638, 0.5081]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.4953, 0.3741, 0.8453, 0.9281],\n",
      "        [0.0750, 0.4365, 0.6578, 0.8825],\n",
      "        [0.1266, 0.2566, 0.7125, 0.6115]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.0133, 0.5699, 0.2867, 0.8853],\n",
      "        [0.2367, 0.5591, 0.4800, 0.8280],\n",
      "        [0.1467, 0.1505, 0.9133, 0.4946],\n",
      "        [0.6367, 0.6344, 0.9500, 0.9606]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.3267, 0.0757, 0.7040, 0.5917],\n",
      "        [0.0015, 0.0688, 0.2883, 0.6422],\n",
      "        [0.1779, 0.0115, 0.5169, 0.5046]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.0080, 0.0740, 0.6800, 0.8000]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.3010, 0.0679, 0.8970, 0.8619]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.2815, 0.0476, 0.9328, 0.9226]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1412, 0.2378, 0.8529, 0.8822]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.0088, 0.0011, 0.8196, 0.6933]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.2833, 0.2139, 0.8667, 0.8278]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.2517, 0.1491, 0.9067, 0.9814]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.0957, 0.1665, 0.6400, 0.7550]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.2296, 0.0063, 0.6633, 0.5938],\n",
      "        [0.3163, 0.1875, 0.9949, 0.6875],\n",
      "        [0.0051, 0.1688, 0.2398, 0.6187],\n",
      "        [0.0867, 0.0437, 0.3929, 0.5688]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.0535, 0.0592, 0.9493, 0.9155]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.3897, 0.2392, 0.9910, 1.0000],\n",
      "        [0.0038, 0.0911, 0.6090, 1.0000]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1457, 0.0817, 0.8657, 0.9281]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.4189, 0.1267, 0.9333, 0.9166]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.0211, 0.0298, 0.9796, 0.9011]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.0373, 0.0660, 0.9420, 0.9460]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1614, 0.3225, 0.8329, 0.8750]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.0021, 0.0030, 0.9518, 0.9260]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.0767, 0.1126, 0.9067, 0.9044]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.0039, 0.1357, 0.9599, 0.9805]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.2335, 0.4556, 0.6028, 0.9667],\n",
      "        [0.6088, 0.4500, 0.9760, 0.9722]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.0925, 0.1364, 0.8303, 0.9489]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.2656, 0.0867, 0.7781, 0.7939],\n",
      "        [0.0844, 0.2670, 0.7063, 0.8689],\n",
      "        [0.1969, 0.0726, 0.6844, 0.5293]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1857, 0.0419, 0.6214, 0.7784],\n",
      "        [0.4964, 0.0299, 0.9821, 0.8922]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1762, 0.2048, 0.6237, 0.6777],\n",
      "        [0.0962, 0.1757, 0.8425, 0.7794]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1961, 0.2328, 0.8782, 0.8266]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.0523, 0.2490, 0.7285, 0.9722]])\n",
      "torch.Size([3, 224, 224]) tensor([[3.0494e-01, 9.2593e-04, 8.9630e-01, 6.8611e-01],\n",
      "        [5.0741e-01, 5.9537e-01, 1.0000e+00, 9.8704e-01]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1040, 0.2240, 0.8880, 0.9560]])\n",
      "torch.Size([3, 224, 224]) tensor([[3.9199e-01, 2.3887e-01, 8.2736e-01, 7.3442e-01],\n",
      "        [5.2377e-01, 1.4837e-03, 9.7498e-01, 4.4214e-01],\n",
      "        [8.3403e-04, 4.4510e-03, 3.4362e-01, 5.1039e-01],\n",
      "        [8.3403e-04, 3.6944e-01, 2.1685e-01, 9.9258e-01],\n",
      "        [1.9099e-01, 6.6617e-01, 5.9216e-01, 1.0000e+00],\n",
      "        [6.8557e-01, 7.1217e-01, 9.9917e-01, 1.0000e+00]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1005, 0.1917, 0.8854, 0.9102]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.0280, 0.0211, 0.5560, 0.7368]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1167, 0.3764, 0.4075, 0.8381],\n",
      "        [0.3242, 0.4228, 0.6817, 0.8632],\n",
      "        [0.5175, 0.3476, 0.8042, 0.7779],\n",
      "        [0.3042, 0.0765, 0.6708, 0.5257]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.0956, 0.0491, 0.7778, 0.8248]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.5840, 0.6186, 0.8670, 1.0000],\n",
      "        [0.4920, 0.1992, 0.7400, 0.5565],\n",
      "        [0.1760, 0.2811, 0.4900, 0.6582],\n",
      "        [0.3670, 0.0240, 0.6190, 0.3390],\n",
      "        [0.6420, 0.0494, 0.9070, 0.3799]])\n",
      "torch.Size([3, 224, 224]) tensor([[3.0500e-01, 3.1944e-01, 9.7750e-01, 8.4111e-01],\n",
      "        [3.8333e-02, 2.9111e-01, 8.4250e-01, 6.6778e-01],\n",
      "        [8.3333e-04, 2.2167e-01, 7.4917e-01, 4.9389e-01],\n",
      "        [1.3667e-01, 1.1778e-01, 7.8750e-01, 4.3944e-01]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1825, 0.0113, 0.8125, 1.0000]])\n",
      "torch.Size([3, 224, 224]) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1463, 0.2129, 0.4642, 0.7479],\n",
      "        [0.5884, 0.3595, 0.8821, 0.7905],\n",
      "        [0.2653, 0.4634, 0.5568, 0.8756],\n",
      "        [0.7358, 0.3152, 0.9884, 0.7717]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1100, 0.1026, 0.8800, 0.8462]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.5168, 0.1902, 0.9804, 0.9498]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1233, 0.1182, 0.8490, 1.0000]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.3313, 0.2281, 0.7083, 0.7984]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.4593, 0.3480, 0.9971, 1.0000]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.0833, 0.1400, 0.9167, 0.9900]])\n",
      "torch.Size([3, 224, 224]) tensor([[9.7173e-02, 4.3042e-01, 8.0654e-01, 9.1450e-01],\n",
      "        [8.8339e-04, 5.8962e-04, 3.5954e-01, 3.6380e-01],\n",
      "        [9.1873e-02, 2.3585e-03, 5.5124e-01, 3.2842e-01]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1900, 0.4540, 0.5240, 0.8080],\n",
      "        [0.4580, 0.4380, 0.8120, 0.7520],\n",
      "        [0.3380, 0.2560, 0.6960, 0.5940]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1625, 0.4916, 0.3675, 0.7730],\n",
      "        [0.1950, 0.0826, 0.4338, 0.4503],\n",
      "        [0.4212, 0.0732, 0.5788, 0.7992],\n",
      "        [0.4737, 0.2946, 0.8037, 0.9099],\n",
      "        [0.4875, 0.0713, 0.6800, 0.7786]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.3175, 0.2037, 0.7275, 0.6087],\n",
      "        [0.2713, 0.5600, 0.6687, 0.8913],\n",
      "        [0.7538, 0.5875, 1.0000, 0.8950],\n",
      "        [0.5850, 0.2237, 0.9087, 0.5838],\n",
      "        [0.0012, 0.0787, 0.3850, 0.5175]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1439, 0.1909, 0.9045, 0.8061]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1299, 0.2346, 0.4851, 0.7106],\n",
      "        [0.5179, 0.2175, 0.9000, 0.6815]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.2542, 0.2806, 0.6906, 0.8333],\n",
      "        [0.6917, 0.1778, 0.9573, 0.5125]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1037, 0.0722, 0.4630, 0.6134]])\n",
      "torch.Size([3, 224, 224]) tensor([[0.1437, 0.1914, 0.9453, 0.9815]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    }
   ],
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    #A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(p=1.0),\n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['labels']))\n",
    "\n",
    "dataset = FruitDataset(data_dir='/Users/matvejzasadko/Downloads/All/Study/NNetworks/Lb1/archive/train_zip/train', transforms=transform, image_size=(224, 224))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "for images, targets in dataloader:\n",
    "    print(images[0].shape, targets[0]['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f3b4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO(nn.Module):\n",
    "    def __init__(self, in_channels=3, split_size=10, num_boxes=2, num_classes=3):\n",
    "        super(YOLO, self).__init__()\n",
    "        self.S = split_size\n",
    "        self.B = num_boxes\n",
    "        self.C = num_classes\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(192, 128, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            *[\n",
    "                nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                \n",
    "                nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.LeakyReLU(0.1)\n",
    "            ] * 4,\n",
    "            \n",
    "            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "        self._calculate_conv_output()\n",
    "\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.conv_output_size, 4096),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(4096, self.S * self.S * (self.C + self.B * 5))\n",
    "        )\n",
    "\n",
    "    def _calculate_conv_output(self):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 224, 224)\n",
    "            conv_output = self.conv_layers(dummy_input)\n",
    "            self.conv_output_size = conv_output.view(-1).size(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fcs(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bacd261b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 392])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "S, B, C = 7, 1, 3\n",
    "\n",
    "model = YOLO(in_channels=3, split_size=S, num_boxes=B, num_classes=C).to(device)\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "output = model(dummy_input)\n",
    "print(output.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1644015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels, box_format='midpoint'):\n",
    " \n",
    "    if box_format == 'midpoint':\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "    if box_format == 'corners':\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4] \n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "    \n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "    \n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    \n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "    \n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a4498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, S=7, B=1, C=3, lambda_coord=5):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.lambda_coord = lambda_coord\n",
    "\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
    "\n",
    "        if self.B >= 2:\n",
    "            iou_b1 = intersection_over_union(predictions[..., self.C + 1:self.C + 5], target[..., self.C + 1:self.C + 5])\n",
    "            iou_b2 = intersection_over_union(predictions[..., self.C + 6:self.C + 10], target[..., self.C + 1:self.C + 5])\n",
    "            ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "            iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        else:\n",
    "            iou_b1 = intersection_over_union(predictions[..., self.C + 1:self.C + 5], target[..., self.C + 1:self.C + 5])\n",
    "            iou_maxes = iou_b1\n",
    "            bestbox = torch.zeros_like(iou_b1, dtype=torch.bool)  \n",
    "     \n",
    "        exists_box = target[..., self.C].unsqueeze(3)\n",
    "\n",
    "        box_predictions = exists_box * (\n",
    "            (bestbox * predictions[..., self.C + 6:self.C + 10] if self.B >= 2 else predictions[..., self.C + 1:self.C + 5])\n",
    "        )\n",
    "        box_targets = exists_box * target[..., self.C + 1:self.C + 5]\n",
    "\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "ї\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2),\n",
    "            torch.flatten(exists_box * target[..., :self.C], end_dim=-2),\n",
    "        )\n",
    "\n",
    "        return class_loss, self.lambda_coord * box_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "983882fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Class Loss: 1.9155, Box Loss: 25.0652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Class Loss: 1.2205, Box Loss: 16.6648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Class Loss: 1.0526, Box Loss: 15.2889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Class Loss: 0.9167, Box Loss: 13.3029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Class Loss: 0.8916, Box Loss: 13.1018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Class Loss: 0.8692, Box Loss: 12.4108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Class Loss: 0.8041, Box Loss: 11.1631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Class Loss: 0.7315, Box Loss: 10.6619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Class Loss: 0.7399, Box Loss: 9.4557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Class Loss: 0.6507, Box Loss: 9.2914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Class Loss: 0.6352, Box Loss: 9.1030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Class Loss: 0.6046, Box Loss: 8.5568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Class Loss: 0.6352, Box Loss: 8.0774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Class Loss: 0.5534, Box Loss: 7.9955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Class Loss: 0.5722, Box Loss: 7.6664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Class Loss: 0.5458, Box Loss: 7.6368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Class Loss: 0.5584, Box Loss: 6.9575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Class Loss: 0.5322, Box Loss: 7.3211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Class Loss: 0.5602, Box Loss: 7.4001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Class Loss: 0.5671, Box Loss: 7.5399\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.00001\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "WEIGHT_DECAY = 0.0003\n",
    "EPOCHS = 20\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"model.pth\"\n",
    "\n",
    "S, B, C = 7, 1, 3\n",
    "\n",
    "model = YOLO(in_channels=3, split_size=S, num_boxes=B, num_classes=C).to(DEVICE)\n",
    "criterion = YoloLoss(S=S, B=B, C=C)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_class_loss = 0\n",
    "    total_box_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        images, targets = batch  \n",
    "        \n",
    "        if isinstance(images, tuple):\n",
    "            images = torch.stack(images)\n",
    "        \n",
    "        images = images.to(DEVICE)\n",
    "        \n",
    "        target_tensor = torch.zeros((len(images), S, S, C + B * 5), device=DEVICE)\n",
    "        for i, target in enumerate(targets):\n",
    "            for box, label in zip(target['boxes'], target['labels']):\n",
    "                x_center = (box[0] + box[2]) / 2 * S  \n",
    "                y_center = (box[1] + box[3]) / 2 * S\n",
    "                grid_x, grid_y = int(x_center), int(y_center)\n",
    "\n",
    "                target_tensor[i, grid_y, grid_x, :2] = torch.tensor([x_center - grid_x, y_center - grid_y], device=DEVICE)\n",
    "                target_tensor[i, grid_y, grid_x, 2:4] = box[2:] - box[:2]\n",
    "                target_tensor[i, grid_y, grid_x, 4] = 1.0  \n",
    "                target_tensor[i, grid_y, grid_x, 5 + int(label.item())] = 1.0  \n",
    "\n",
    "        predictions = model(images)\n",
    "        \n",
    "        class_loss, box_loss = criterion(predictions, target_tensor)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        (class_loss + box_loss).backward()  \n",
    "        optimizer.step()\n",
    "\n",
    "        total_class_loss += class_loss.item()\n",
    "        total_box_loss += box_loss.item()\n",
    "    \n",
    "    avg_class_loss = total_class_loss / len(dataloader)\n",
    "    avg_box_loss = total_box_loss / len(dataloader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Class Loss: {avg_class_loss:.4f}, Box Loss: {avg_box_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b538dd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 66.67%, Test Box Loss: 10.2388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    images = torch.stack(images, 0)  \n",
    "    return images, targets \n",
    "\n",
    "test_dataset = FruitDataset(\n",
    "    data_dir='/Users/matvejzasadko/Downloads/All/Study/NNetworks/Lb1/archive/test_zip/test', \n",
    "    transforms=transform,\n",
    "    image_size=(224, 224)\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model.eval()  \n",
    "total_correct_classifications = 0\n",
    "total_objects = 0\n",
    "total_box_loss = 0\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():  \n",
    "    for images, targets in test_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        \n",
    "        target_tensor = torch.zeros((len(images), S, S, C + B * 5), device=DEVICE)\n",
    "        for i, target in enumerate(targets):\n",
    "            for box, label in zip(target['boxes'], target['labels']):\n",
    "                x_center = (box[0] + box[2]) / 2 * S  \n",
    "                y_center = (box[1] + box[3]) / 2 * S\n",
    "                grid_x, grid_y = int(x_center), int(y_center)\n",
    "\n",
    "                target_tensor[i, grid_y, grid_x, :2] = torch.tensor([x_center - grid_x, y_center - grid_y], device=DEVICE)\n",
    "                target_tensor[i, grid_y, grid_x, 2:4] = box[2:] - box[:2]\n",
    "                target_tensor[i, grid_y, grid_x, 4] = 1.0  \n",
    "                target_tensor[i, grid_y, grid_x, 5 + int(label.item())] = 1.0  \n",
    "\n",
    "        predictions = model(images).view(-1, S, S, C + B * 5)\n",
    "        \n",
    "        for i in range(len(images)):\n",
    "            for grid_y in range(S):\n",
    "                for grid_x in range(S):\n",
    "                    if target_tensor[i, grid_y, grid_x, 4] == 1: \n",
    "                        true_class = torch.argmax(target_tensor[i, grid_y, grid_x, 5:])\n",
    "                        predicted_class = torch.argmax(predictions[i, grid_y, grid_x, 5:])\n",
    "                        \n",
    "                        if predicted_class == true_class:\n",
    "                            total_correct_classifications += 1\n",
    "                        total_objects += 1\n",
    "        \n",
    "        _, box_loss = criterion(predictions, target_tensor)\n",
    "        total_box_loss += box_loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "accuracy = (total_correct_classifications / total_objects) * 100 if total_objects > 0 else 0\n",
    "avg_box_loss = total_box_loss / num_batches\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%, Test Box Loss: {avg_box_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137704e3",
   "metadata": {},
   "source": [
    "Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c258aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO_Tune(nn.Module):\n",
    "    def __init__(self, in_channels=3, split_size=10, num_boxes=2, num_classes=3,\n",
    "                 kernel_sizes=[7, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3],\n",
    "                 strides=[2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1],\n",
    "                 paddings=[3, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1],\n",
    "                 num_filters=[64, 192, 128, 256, 256, 512, 512, 1024, 1024, 1024, 1024, 1024],\n",
    "                 dropout_rate=0.5):\n",
    "        super(YOLO_Tune, self).__init__()\n",
    "        self.S = split_size\n",
    "        self.B = num_boxes\n",
    "        self.C = num_classes\n",
    "        \n",
    "        layers = []\n",
    "        in_channels_current = in_channels\n",
    "        input_size = 224  \n",
    "\n",
    "        for i in range(len(kernel_sizes)):\n",
    "            layers.append(nn.Conv2d(in_channels_current, num_filters[i], kernel_size=kernel_sizes[i], \n",
    "                                    stride=strides[i], padding=paddings[i]))\n",
    "            layers.append(nn.BatchNorm2d(num_filters[i]))\n",
    "            layers.append(nn.LeakyReLU(0.1))\n",
    "\n",
    "            input_size = (input_size + 2 * paddings[i] - kernel_sizes[i]) // strides[i] + 1\n",
    "\n",
    "            if input_size > 2 and (i + 1) % 2 == 0: \n",
    "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "                input_size //= 2\n",
    "\n",
    "            if input_size <= 1:\n",
    "                print(\"Skipping additional layers to prevent 1x1 spatial dimensions.\")\n",
    "                break\n",
    "\n",
    "            in_channels_current = num_filters[i]\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "\n",
    "        self._calculate_conv_output()\n",
    "\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.conv_output_size, 4096),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(4096, self.S * self.S * (self.C + self.B * 5))\n",
    "        )\n",
    "\n",
    "    def _calculate_conv_output(self):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 224, 224)\n",
    "            conv_output = self.conv_layers(dummy_input)\n",
    "            self.conv_output_size = conv_output.view(-1).size(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fcs(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f16824d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping additional layers to prevent 1x1 spatial dimensions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [1/10], Epoch [1/3], Class Loss: 3.3731, Box Loss: 31.2203, Total Loss: 34.5934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [1/10], Epoch [2/3], Class Loss: 1.7951, Box Loss: 20.5161, Total Loss: 22.3112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [1/10], Epoch [3/3], Class Loss: 1.3063, Box Loss: 18.0510, Total Loss: 19.3572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [2/10], Epoch [1/3], Class Loss: 2.2033, Box Loss: 23.1581, Total Loss: 25.3614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [2/10], Epoch [2/3], Class Loss: 1.2882, Box Loss: 14.5840, Total Loss: 15.8722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [2/10], Epoch [3/3], Class Loss: 0.9645, Box Loss: 11.9238, Total Loss: 12.8882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [3/10], Epoch [1/3], Class Loss: 2.0121, Box Loss: 23.6266, Total Loss: 25.6387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [3/10], Epoch [2/3], Class Loss: 1.0120, Box Loss: 12.8223, Total Loss: 13.8343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [3/10], Epoch [3/3], Class Loss: 0.7606, Box Loss: 10.5672, Total Loss: 11.3278\n",
      "Skipping additional layers to prevent 1x1 spatial dimensions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [4/10], Epoch [1/3], Class Loss: 2.1161, Box Loss: 28.1050, Total Loss: 30.2211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [4/10], Epoch [2/3], Class Loss: 1.2176, Box Loss: 16.0849, Total Loss: 17.3025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [4/10], Epoch [3/3], Class Loss: 0.9924, Box Loss: 13.8509, Total Loss: 14.8432\n",
      "Skipping additional layers to prevent 1x1 spatial dimensions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [5/10], Epoch [1/3], Class Loss: 2.7224, Box Loss: 31.3687, Total Loss: 34.0910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [5/10], Epoch [2/3], Class Loss: 1.7107, Box Loss: 20.3930, Total Loss: 22.1037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [5/10], Epoch [3/3], Class Loss: 1.3340, Box Loss: 17.6060, Total Loss: 18.9400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [6/10], Epoch [1/3], Class Loss: 1.8628, Box Loss: 22.4842, Total Loss: 24.3470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [6/10], Epoch [2/3], Class Loss: 1.0108, Box Loss: 12.7928, Total Loss: 13.8037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [6/10], Epoch [3/3], Class Loss: 0.7556, Box Loss: 10.1569, Total Loss: 10.9125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [7/10], Epoch [1/3], Class Loss: 2.2052, Box Loss: 25.5661, Total Loss: 27.7713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [7/10], Epoch [2/3], Class Loss: 1.2455, Box Loss: 15.3820, Total Loss: 16.6276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [7/10], Epoch [3/3], Class Loss: 0.9895, Box Loss: 13.1469, Total Loss: 14.1364\n",
      "Skipping additional layers to prevent 1x1 spatial dimensions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [8/10], Epoch [1/3], Class Loss: 2.1114, Box Loss: 26.5045, Total Loss: 28.6159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [8/10], Epoch [2/3], Class Loss: 1.2101, Box Loss: 16.1041, Total Loss: 17.3142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [8/10], Epoch [3/3], Class Loss: 0.9172, Box Loss: 12.8537, Total Loss: 13.7710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [9/10], Epoch [1/3], Class Loss: 2.1078, Box Loss: 25.7052, Total Loss: 27.8130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [9/10], Epoch [2/3], Class Loss: 1.3360, Box Loss: 16.4837, Total Loss: 17.8197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [9/10], Epoch [3/3], Class Loss: 1.0679, Box Loss: 14.2637, Total Loss: 15.3315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [10/10], Epoch [1/3], Class Loss: 1.6315, Box Loss: 21.7490, Total Loss: 23.3805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [10/10], Epoch [2/3], Class Loss: 0.9003, Box Loss: 12.8504, Total Loss: 13.7507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial [10/10], Epoch [3/3], Class Loss: 0.6609, Box Loss: 10.4107, Total Loss: 11.0716\n",
      "Best parameters found:\n",
      "{'kernel_sizes': [7, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3], 'strides': [2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1], 'paddings': [3, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1], 'num_filters': [32, 64, 128, 256, 256, 512, 512, 1024, 1024, 1024, 1024, 1024], 'dropout_rate': 0.3}\n",
      "Best total loss: 10.912496860325337\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "param_grid = {\n",
    "    'kernel_sizes': [[7, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3], [5, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3]],\n",
    "    'strides': [[2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1], [1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]],\n",
    "    'paddings': [[3, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1], [2, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1]],\n",
    "    'num_filters': [[64, 192, 128, 256, 256, 512, 512, 1024, 1024, 1024, 1024, 1024], [32, 64, 128, 256, 256, 512, 512, 1024, 1024, 1024, 1024, 1024]],\n",
    "    'dropout_rate': [0.3, 0.5]\n",
    "}\n",
    "\n",
    "num_trials = 10\n",
    "best_loss = float(\"inf\")\n",
    "best_params = None\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "num_trials = 10\n",
    "best_loss = float(\"inf\")\n",
    "best_params = None\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    params = {\n",
    "        'kernel_sizes': random.choice([[7, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3], [5, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3]]),\n",
    "        'strides': random.choice([[2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1], [1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]]),\n",
    "        'paddings': random.choice([[3, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1], [2, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1]]),\n",
    "        'num_filters': random.choice([[64, 192, 128, 256, 256, 512, 512, 1024, 1024, 1024, 1024, 1024], [32, 64, 128, 256, 256, 512, 512, 1024, 1024, 1024, 1024, 1024]]),\n",
    "        'dropout_rate': random.choice([0.3, 0.5])\n",
    "    }\n",
    "\n",
    "    model = YOLO_Tune(\n",
    "        in_channels=3,\n",
    "        split_size=S,\n",
    "        num_boxes=B,\n",
    "        num_classes=C,\n",
    "        kernel_sizes=params['kernel_sizes'],\n",
    "        strides=params['strides'],\n",
    "        paddings=params['paddings'],\n",
    "        num_filters=params['num_filters'],\n",
    "        dropout_rate=params['dropout_rate']\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    try:\n",
    "        model._calculate_conv_output()\n",
    "    except ValueError:\n",
    "        print(\"Skipped configuration due to incompatible spatial dimensions.\")\n",
    "        continue  \n",
    "\n",
    "    criterion = YoloLoss(S=S, B=B, C=C)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_class_loss = 0\n",
    "        total_box_loss = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            images, targets = batch  \n",
    "            \n",
    "            if isinstance(images, tuple):\n",
    "                images = torch.stack(images)\n",
    "            \n",
    "            images = images.to(DEVICE)\n",
    "            \n",
    "            target_tensor = torch.zeros((len(images), S, S, C + B * 5), device=DEVICE)\n",
    "            for i, target in enumerate(targets):\n",
    "                for box, label in zip(target['boxes'], target['labels']):\n",
    "                    x_center = (box[0] + box[2]) / 2 * S  \n",
    "                    y_center = (box[1] + box[3]) / 2 * S\n",
    "                    grid_x, grid_y = int(x_center), int(y_center)\n",
    "\n",
    "                    target_tensor[i, grid_y, grid_x, :2] = torch.tensor([x_center - grid_x, y_center - grid_y], device=DEVICE)\n",
    "                    target_tensor[i, grid_y, grid_x, 2:4] = box[2:] - box[:2]\n",
    "                    target_tensor[i, grid_y, grid_x, 4] = 1.0  \n",
    "                    target_tensor[i, grid_y, grid_x, 5 + int(label.item())] = 1.0  \n",
    "\n",
    "            predictions = model(images)\n",
    "            \n",
    "            class_loss, box_loss = criterion(predictions, target_tensor)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            (class_loss + box_loss).backward()  \n",
    "            optimizer.step()\n",
    "\n",
    "            total_class_loss += class_loss.item()\n",
    "            total_box_loss += box_loss.item()\n",
    "        \n",
    "        avg_class_loss = total_class_loss / len(dataloader)\n",
    "        avg_box_loss = total_box_loss / len(dataloader)\n",
    "        total_loss = avg_class_loss + avg_box_loss\n",
    "\n",
    "        print(f\"Trial [{trial+1}/{num_trials}], Epoch [{epoch+1}/{EPOCHS}], Class Loss: {avg_class_loss:.4f}, Box Loss: {avg_box_loss:.4f}, Total Loss: {total_loss:.4f}\")\n",
    "\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        best_params = params\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "print(best_params)\n",
    "print(\"Best total loss:\", best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5060733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.0001\n",
    "# model = YOLO_Tune(\n",
    "#         in_channels=3,\n",
    "#         split_size=S,\n",
    "#         num_boxes=B,\n",
    "#         num_classes=C,\n",
    "#         kernel_sizes=[7, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3],\n",
    "#         strides=[2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1],\n",
    "#         paddings=[3, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1],\n",
    "#         num_filters=[32, 64, 128, 256, 256, 512, 512, 1024, 1024, 1024, 1024, 1024],\n",
    "#         dropout_rate=0.3\n",
    "#     ).to(DEVICE)\n",
    "\n",
    "model2 = YOLO_Tune(\n",
    "        in_channels=3,\n",
    "        split_size=S,\n",
    "        num_boxes=B,\n",
    "        num_classes=C,\n",
    "        kernel_sizes=[7, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3],\n",
    "        strides=[2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1],\n",
    "        paddings=[3, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1],\n",
    "        num_filters=[32, 64, 128, 256, 256, 512, 512, 1024, 1024, 1024, 1024, 1024],\n",
    "        dropout_rate=0.3\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad79a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = YoloLoss(S=S, B=B, C=C)\n",
    "optimizer = optim.Adam(model2.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "80883789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Class Loss: 1.2698, Box Loss: 30.8198, Total Loss: 32.0896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Class Loss: 1.0786, Box Loss: 26.8031, Total Loss: 27.8817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Class Loss: 0.8770, Box Loss: 20.1121, Total Loss: 20.9891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Class Loss: 0.8538, Box Loss: 18.0183, Total Loss: 18.8721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Class Loss: 0.6843, Box Loss: 13.8234, Total Loss: 14.5076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Class Loss: 0.6485, Box Loss: 13.5138, Total Loss: 14.1623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Class Loss: 0.6085, Box Loss: 11.6768, Total Loss: 12.2852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Class Loss: 0.7660, Box Loss: 13.4749, Total Loss: 14.2409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Class Loss: 0.6541, Box Loss: 10.9532, Total Loss: 11.6072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Class Loss: 0.5667, Box Loss: 9.8841, Total Loss: 10.4508\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "        model2.train()\n",
    "        total_class_loss = 0\n",
    "        total_box_loss = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            images, targets = batch  \n",
    "            \n",
    "            if isinstance(images, tuple):\n",
    "                images = torch.stack(images)\n",
    "            \n",
    "            images = images.to(DEVICE)\n",
    "            \n",
    "            target_tensor = torch.zeros((len(images), S, S, C + B * 5), device=DEVICE)\n",
    "            for i, target in enumerate(targets):\n",
    "                for box, label in zip(target['boxes'], target['labels']):\n",
    "                    x_center = (box[0] + box[2]) / 2 * S  \n",
    "                    y_center = (box[1] + box[3]) / 2 * S\n",
    "                    grid_x, grid_y = int(x_center), int(y_center)\n",
    "\n",
    "                    target_tensor[i, grid_y, grid_x, :2] = torch.tensor([x_center - grid_x, y_center - grid_y], device=DEVICE)\n",
    "                    target_tensor[i, grid_y, grid_x, 2:4] = box[2:] - box[:2]\n",
    "                    target_tensor[i, grid_y, grid_x, 4] = 1.0  \n",
    "                    target_tensor[i, grid_y, grid_x, 5 + int(label.item())] = 1.0  \n",
    "\n",
    "            predictions = model2(images)\n",
    "            \n",
    "            class_loss, box_loss = criterion(predictions, target_tensor)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            (class_loss + box_loss).backward()  \n",
    "            optimizer.step()\n",
    "\n",
    "            total_class_loss += class_loss.item()\n",
    "            total_box_loss += box_loss.item()\n",
    "        avg_class_loss = total_class_loss / len(dataloader)\n",
    "        avg_box_loss = total_box_loss / len(dataloader)\n",
    "        total_loss = avg_class_loss + avg_box_loss\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Class Loss: {avg_class_loss:.4f}, Box Loss: {avg_box_loss:.4f}, Total Loss: {total_loss:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "14704ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 49.12%, Test Box Loss: 15.7766\n",
      "Accuracy for class 0: 68.57%\n",
      "Accuracy for class 1: 45.95%\n",
      "Accuracy for class 2: 35.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    images = torch.stack(images, 0)  \n",
    "    return images, targets \n",
    "\n",
    "test_dataset = FruitDataset(\n",
    "    data_dir='/Users/matvejzasadko/Downloads/All/Study/NNetworks/Lb1/archive/test_zip/test', \n",
    "    transforms=transform,\n",
    "    image_size=(224, 224)\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model2.eval()  \n",
    "total_correct_classifications = 0\n",
    "total_objects = 0\n",
    "total_box_loss = 0\n",
    "num_batches = 0\n",
    "\n",
    "correct_per_class = defaultdict(int)\n",
    "total_per_class = defaultdict(int)\n",
    "\n",
    "with torch.no_grad():  \n",
    "    for images, targets in test_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        \n",
    "        target_tensor = torch.zeros((len(images), S, S, C + B * 5), device=DEVICE)\n",
    "        for i, target in enumerate(targets):\n",
    "            for box, label in zip(target['boxes'], target['labels']):\n",
    "                x_center = (box[0] + box[2]) / 2 * S  \n",
    "                y_center = (box[1] + box[3]) / 2 * S\n",
    "                grid_x, grid_y = int(x_center), int(y_center)\n",
    "\n",
    "                target_tensor[i, grid_y, grid_x, :2] = torch.tensor([x_center - grid_x, y_center - grid_y], device=DEVICE)\n",
    "                target_tensor[i, grid_y, grid_x, 2:4] = box[2:] - box[:2]\n",
    "                target_tensor[i, grid_y, grid_x, 4] = 1.0  \n",
    "                target_tensor[i, grid_y, grid_x, 5 + int(label.item())] = 1.0  \n",
    "\n",
    "        predictions = model(images).view(-1, S, S, C + B * 5)\n",
    "        \n",
    "        for i in range(len(images)):\n",
    "            for grid_y in range(S):\n",
    "                for grid_x in range(S):\n",
    "                    if target_tensor[i, grid_y, grid_x, 4] == 1: \n",
    "                        true_class = torch.argmax(target_tensor[i, grid_y, grid_x, 5:])\n",
    "                        predicted_class = torch.argmax(predictions[i, grid_y, grid_x, 5:])\n",
    "                        \n",
    "                        if predicted_class == true_class:\n",
    "                            total_correct_classifications += 1\n",
    "                            correct_per_class[true_class.item()] += 1\n",
    "                        total_objects += 1\n",
    "                        total_per_class[true_class.item()] += 1\n",
    "        \n",
    "        _, box_loss = criterion(predictions, target_tensor)\n",
    "        total_box_loss += box_loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "accuracy = (total_correct_classifications / total_objects) * 100 if total_objects > 0 else 0\n",
    "avg_box_loss = total_box_loss / num_batches\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%, Test Box Loss: {avg_box_loss:.4f}\")\n",
    "\n",
    "for class_id in range(C):\n",
    "    if total_per_class[class_id] > 0:\n",
    "        class_accuracy = (correct_per_class[class_id] / total_per_class[class_id]) * 100\n",
    "        print(f\"Accuracy for class {class_id}: {class_accuracy:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Accuracy for class {class_id}: No samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7b2cd",
   "metadata": {},
   "source": [
    "Нажаль, модель типу YOLO не дуже справилась з завданням object detection, на такій маленькій кількості даних\n",
    "Точність на тестових даних склала всього 66%\n",
    "Після підбору гіперпараметрів схоже, відбувся overfitting, модель показала значно кращі результати на тренувальних даних, проте на тестових даних точністі стала меншою\n",
    "Найбільше модель помиляється на даних класу 2(банани), це через те, що зображення бананівв найбільш різноманітні, і не вистачає картинок, щоб модель до цього пристосувалась\n",
    "Найкращий сет гіперпараметрів {'kernel_sizes': [7, 3, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3], 'strides': [2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1], 'paddings': [3, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1], 'num_filters': [32, 64, 128, 256, 256, 512, 512, 1024, 1024, 1024, 1024, 1024], 'dropout_rate': 0.3};\n",
    "\n",
    "Kernel size - розмір згорткового ядра;   \n",
    "Strides - крок згортки;  \n",
    "Peddings - скільки пікселів буде додано навколо меж зображення перед застосуванням згортки;  \n",
    "num_filters - скільки різних фільтрів буде застосовано до вхідного зображення або попереднього шару для виділення різних ознак;\n",
    "\n",
    "Strides - визначає ймовірність \"відключення\" випадкових нейронів у шарі під час навчання, щоб запобігти перенавчанню;    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f27ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a4cfac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
